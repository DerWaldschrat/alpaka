<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.10"/>
<title>alpaka: C:/dev/alpaka/doc/Rationale.md Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">alpaka
   </div>
   <div id="projectbrief">Abstraction Library for Parallel Kernel Acceleration</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.10 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
      <li><a href="globals.html"><span>File&#160;Members</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('Rationale_8md.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">C:/dev/alpaka/doc/Rationale.md</div>  </div>
</div><!--header-->
<div class="contents">
<a href="Rationale_8md.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;Rationale</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;=========</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;This document gives reasons why some implementation details are the way they are.</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;Kernel Interface</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;----------------</div>
<div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;</div>
<div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;### Requirements</div>
<div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;</div>
<div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;- User kernels should be implemented independent of the accelerator.</div>
<div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;- A user kernel has to have access to accelerator methods like synchronization within blocks, index retrieval and many more.</div>
<div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;- For usage with CUDA the kernel methods have to be attributed with \__device\__ \__host\__.</div>
<div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;- The user kernel has to fulfill std::is_trivially_copyable because only such objects can be copied into CUDA device memory.</div>
<div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;  A trivially copyable class is a class that</div>
<div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;   1. Has no non-trivial copy constructors(this also requires no virtual functions or virtual bases)</div>
<div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;   2. Has no non-trivial move constructors</div>
<div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;   3. Has no non-trivial copy assignment operators</div>
<div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;   4. Has no non-trivial move assignment operators</div>
<div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;   5. Has a trivial destructor</div>
<div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;</div>
<div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;### Implementation variants</div>
<div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;</div>
<div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;There are two possible ways to tell the kernel about the accelerator type:</div>
<div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160; 1. The kernel is templated on the accelerator type</div>
<div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;  * + This allows users to specialize them for different accelerators. (Is this is really necessary or desired?)</div>
<div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;  * - The kernel has to be a class template. This does not allow C++ lambdas to be used as kernels because they are no templates themselves (but only their `operator()` can be templated in C++14).</div>
<div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;  * - This prevents the user from instantiating an accelerator independent kernel before executing it.</div>
<div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;  Because the memory layout in inheritance hierarchies is undefined a simple copy of the user kernel or its members to its specialized type is not possible platform independently.</div>
<div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;  This would require a copy from UserKernel&lt;TDummyAcc&gt; to UserKernel&lt;TAcc&gt; to be possible.</div>
<div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;  The only way to allow this would be to require the user to implement a templated copy constructor for every kernel.</div>
<div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;  This is not allowed for kernels that should be copyable to a CUDA device because std::is_trivially_copyable requires the kernel to have no non-trivial copy constructors.</div>
<div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;  * a) and inherits from the accelerator. </div>
<div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;    * +/- To give a device function called from the kernel function object access to the accelerator methods, these methods have to be templated on the kernel function object and get a reference to the accelerator.</div>
<div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;    This allows to give them access not only to the accelerator methods but also to the other kernel methods.</div>
<div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;    This is inconsistent because the kernel uses inheritance and subsequent function calls get a parameter.</div>
<div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;    * - The kernel itself has to inherit at least protected from the accelerator to allow the KernelExecutor to access the Accelerator.</div>
<div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;    * - How do accelerator functions called from the kernel (and not within the kernel class itself) access the accelerator methods?</div>
<div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;    Casting this to the accelerator type and giving it as parameter is too much to require from the user.</div>
<div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;  * b) and the `operator()` has a reference to the accelerator as parameter.</div>
<div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;    * + This allows to use the accelerator in accelerator functions called from the kernel (and not within the kernel class itself) to access the accelerator methods in the same way the kernel entry point function can.</div>
<div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;    * - This would require an additional object (the accelerator) in device memory taking up valuable CUDA registers (opposed to the inheritance solution). At least on CUDA all the accelerator functions could be inlined nevertheless.</div>
<div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160; 2. The `operator()` is templated on the accelerator type and has a reference to the accelerator as parameter.</div>
<div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;  * + The kernel can be an arbitrary function object with ALPAKA_FCT_HOST_ACC attributes.</div>
<div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;  * + This would allow to instantiate the accelerator independent kernel and set its members before execution.</div>
<div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;  * +/- C++14 provides polymorphic lambdas. All compilers (even MSVC) support this. Inheriting from a non capturing lambda for the KernelExecutor is allowed. (TODO: How to check for a non capturing lambda?)</div>
<div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;  * - The `operator()` could be overloaded on the accelerator type but not the kernel itself, so it always has the same members.</div>
<div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;  * - This would require an additional object (the accelerator) in device memory taking up valuable CUDA registers (opposed to the inheritance solution). At least on CUDA all the accelerator functions could be inlined nevertheless.</div>
<div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;</div>
<div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;### Implementation notes</div>
<div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;</div>
<div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;Currently we implement version 2.</div>
<div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;</div>
<div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;A kernel executor can be obtained by calling `alpaka::exec::create&lt;TAcc&gt;(TWorkDiv, TStream)` with the execution attributes (grid/block-extents, stream).</div>
<div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;This separates the kernel execution attributes (grid/block-extents, stream) from the invocation arguments.</div>
<div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;The returned executor can then be called with the `operator()` leading to `alpaka::exec::create&lt;TAcc&gt;(TWorkDiv, TStream)(invocation-args ...)` for a complete kernel invocation.</div>
<div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160; </div>
<div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;</div>
<div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;Block Shared Memory</div>
<div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;-------------------</div>
<div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160; </div>
<div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;### Internal Block Shared Memory</div>
<div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;</div>
<div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;The size of block shared memory that is allocated inside the kernel is required to be given as compile time constant.</div>
<div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;This is due to CUDA not allowing to allocate block shared memory inside a kernel at runtime.</div>
<div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160; </div>
<div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;### External Block Shared Memory</div>
<div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;</div>
<div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;The size of the external block shared memory is obtained from a trait that can be specialized for each kernel.</div>
<div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;The trait is called with the current kernel invocation parameters and the block size prior to each kernel execution.</div>
<div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;Because the block shared memory size is only ever constant or dependent on the block size or the parameters of the invocation this has multiple advantages:</div>
<div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;* It forces the separation of the kernel invocation from the calculation of the required block shared memory size.</div>
<div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;* It lets the user write this calculation once instead of multiple time spread across the code.</div>
<div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;</div>
<div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;</div>
<div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;Accelerators</div>
<div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;------------</div>
<div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;</div>
<div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;All the accelerators are restricted by the possibilities of CUDA.</div>
<div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;</div>
<div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;The library does not use a common accelerator base class with virtual functions from which all accelerator implementations inherit (run time polymorphism).</div>
<div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;This reduces runtime overhead because everything can be checked at compile time.</div>
<div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;</div>
<div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;**TODO**: Add note about ALPAKA_FCT_HOST_ACC!</div>
<div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;</div>
<div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;</div>
<div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;Accelerator Access within Kernels</div>
<div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;---------------------------------</div>
<div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;</div>
<div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;CUDA always tracks some implicit state like the current device in host code or the current thread and block index in kernel code.</div>
<div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;This implicit state hides dependencies and can produce bugs if the wrong device is active during a memory operation in host code or similar things.</div>
<div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;In alpaka this is always made explicit.</div>
<div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;Streams, events and memory always require a device parameter for their creation.</div>
<div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;</div>
<div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;The kernels have access to the accelerator through a reference parameter.</div>
<div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;There are two possible ways to implement access to accelerator dependent functionality inside a kernel:</div>
<div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;* Making the functions/templates members of the accelerator (maybe by inheritance) and calling them like `acc.syncThreads()` or `acc.template getIdx&lt;Grid, Thread, Dim1&gt;()`.</div>
<div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;This would require the user to know and understand when to use the template keyword inside dependent type  object function calls.</div>
<div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;* The functions are only light wrappers around traits that can be specialized taking the accelerator as first value (it can not be the last value because of the potential use of variadic arguments). </div>
<div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;The resulting code would look like `sync(acc)` or `getIdx&lt;Grid, Thread, Dim1&gt;(acc)`.</div>
<div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;Internally these wrappers would call trait templates that are specialized for the specific accelerator e.g. `template&lt;typename TAcc&gt; Sync{...};`</div>
<div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;</div>
<div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;The second version is easier to understand and usually shorter to use in user code.</div>
<div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;NOTE: Currently version 1 is implemented!</div>
<div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;</div>
<div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;</div>
<div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;Accelerator Implementation Notes</div>
<div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;--------------------------------</div>
<div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;</div>
<div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;### Serial</div>
<div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;</div>
<div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;The serial accelerator only allows blocks with exactly one thread.</div>
<div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;Therefore it does not implement real synchronization or atomic primitives.</div>
<div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;</div>
<div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;### Threads</div>
<div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;</div>
<div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;#### Execution</div>
<div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;</div>
<div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;To prevent recreation of the threads between execution of different blocks in the grid, the threads are stored inside a thread pool.</div>
<div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;This thread pool is local to the invocation because making it local to the KernelExecutor could mean a heavy memory usage and lots of idling kernel-threads when there are multiple KernelExecutors around.</div>
<div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;Because the default policy of the threads in the pool is to yield instead of waiting, this would also slow down the system immensely.</div>
<div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;</div>
<div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;std::thread::hardware_concurrency()</div>
<div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;</div>
<div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;### Fibers</div>
<div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;</div>
<div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;#### Execution</div>
<div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;</div>
<div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;To prevent recreation of the fibers between execution of different blocks in the grid, the fibers are stored inside a fibers pool.</div>
<div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;This fiber pool is local to the invocation because making it local to the KernelExecutor could mean a heavy memory usage when there are multiple KernelExecutors around.</div>
<div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;</div>
<div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;### OpenMP</div>
<div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;</div>
<div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;#### Execution</div>
<div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;</div>
<div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;Parallel execution of the kernels in a block is required because when syncBlockThreads is called all of them have to be done with their work up to this line.</div>
<div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;So we have to spawn one real thread per kernel in a block.</div>
<div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;`omp for` is not useful because it is meant for cases where multiple iterations are executed by one thread but in our case a 1:1 mapping is required.</div>
<div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;Therefore we use `omp parallel` with the specified number of threads in a block.</div>
<div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;Another reason for not using `omp for` like `#pragma omp parallel for collapse(3) num_threads(blockDim.x*blockDim.y*blockDim.z)` is that `#pragma omp barrier` used for intra block synchronization is not allowed inside `omp for` blocks.</div>
<div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;</div>
<div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;Because OpenMP is designed for a 1:1 abstraction of hardware to software threads, the block size is restricted by the number of OpenMP threads allowed by the runtime. </div>
<div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;This could be as little as 2 or 4 kernels but on a system with 4 cores and hyper-threading OpenMP can also allow 64 threads.</div>
<div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;</div>
<div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;::omp_get_max_threads()</div>
<div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;::omp_get_thread_limit()</div>
<div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;</div>
<div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;#### Index</div>
<div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;</div>
<div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;OpenMP only provides a linear thread index. This index is converted to a 3 dimensional index at runtime.</div>
<div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;</div>
<div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;#### Atomic</div>
<div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;</div>
<div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;We can not use &#39;#pragma omp atomic&#39; because braces or calling other functions directly after `#pragma omp atomic` are not allowed.</div>
<div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;Because we are implementing the CUDA atomic operations which return the old value, this requires `#pragma omp critical` to be used.</div>
<div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;`omp_set_lock` is an alternative but is usually slower.</div>
<div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;</div>
<div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;### CUDA</div>
<div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;</div>
<div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;Nearly all CUDA functionality can be directly mapped to alpaka function calls.</div>
<div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;A major difference is that CUDA requires the block and grid sizes to be given in (x, y, z) order.</div>
<div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;Alpaka uses the mathematical C/C++ array indexing scheme [z][y][x].</div>
<div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;Dimension 0 in this case is z, dimensions 2 is x.</div>
<div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;</div>
<div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;Furthermore alpaka does not require the indices and extents to be 3-dimensional.</div>
<div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;The accelerators are templatized on and support arbitrary dimensionality.</div>
<div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;NOTE: Currently the CUDA implementation is restricted to a maximum of 3 dimensions!</div>
<div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;</div>
<div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;NOTE: The CUDA-accelerator back-end can change the current CUDA device and will NOT set the device back to the one prior to the invocation of the alpaka function!</div>
<div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;</div>
<div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;Device Implementations</div>
<div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;----------------------</div>
<div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;</div>
<div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;|-|CPU|CUDA|</div>
<div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;|---|---|---|</div>
<div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;|Devices|(only ever one device)|cudaGetDeviceCount, cudaGetDevice, cudaGetDeviceProperties|</div>
<div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;|Events|std::condition_variable, std::mutex|cudaEventCreateWithFlags, cudaEventDestroy, cudaEventRecord, cudaEventSynchronize, cudaEventQuery|</div>
<div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;|Streams|Thread-Pool with exactly one worker|cudaStreamCreateWithFlags, cudaStreamDestroy, cudaStreamQuery, cudaStreamSynchronize, cudaStreamWaitEvent|</div>
<div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;|Memory|new , delete[], std::memcpy, std::memset, (cudaHostRegister, cudaHostUnregister for memory pinning if available)|cudaMalloc, cudaFree, cudaMemcpy, cudaMemset, cudaHostRegister, cudaHostUnregister, cudaHostGetDevicePointer|</div>
<div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;|RNG|std::mt19937, std::normal_distribution, std::uniform_real_distribution, std::uniform_int_distribution|curand_init, curandStateXORWOW_t, curand, curand_normal, curand_normal_double, curand_uniform, curand_uniform_double|</div>
</div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="Rationale_8md.html">Rationale.md</a></li>
    <li class="footer">Generated on Sat Jul 18 2015 20:35:47 for alpaka by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.10 </li>
  </ul>
</div>
</body>
</html>
